/**
 * Generated by Kubb (https://kubb.dev/).
 * Do not edit manually.
 */

export const LLMConfigModelEndpointTypeEnum = {
  openai: 'openai',
  anthropic: 'anthropic',
  cohere: 'cohere',
  google_ai: 'google_ai',
  google_vertex: 'google_vertex',
  azure: 'azure',
  groq: 'groq',
  ollama: 'ollama',
  webui: 'webui',
  'webui-legacy': 'webui-legacy',
  lmstudio: 'lmstudio',
  'lmstudio-legacy': 'lmstudio-legacy',
  'lmstudio-chatcompletions': 'lmstudio-chatcompletions',
  llamacpp: 'llamacpp',
  koboldcpp: 'koboldcpp',
  vllm: 'vllm',
  'hugging-face': 'hugging-face',
  mistral: 'mistral',
  together: 'together',
  bedrock: 'bedrock',
  deepseek: 'deepseek',
  xai: 'xai',
} as const;

export type LLMConfigModelEndpointTypeEnum =
  (typeof LLMConfigModelEndpointTypeEnum)[keyof typeof LLMConfigModelEndpointTypeEnum];

/**
 * @description Configuration for a Language Model (LLM) model. This object specifies all the information necessary to access an LLM model to usage with Letta, except for secret keys.\n\nAttributes:\n    model (str): The name of the LLM model.\n    model_endpoint_type (str): The endpoint type for the model.\n    model_endpoint (str): The endpoint for the model.\n    model_wrapper (str): The wrapper for the model. This is used to wrap additional text around the input/output of the model. This is useful for text-to-text completions, such as the Completions API in OpenAI.\n    context_window (int): The context window size for the model.\n    put_inner_thoughts_in_kwargs (bool): Puts `inner_thoughts` as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts.\n    temperature (float): The temperature to use when generating text with the model. A higher temperature will result in more random text.\n    max_tokens (int): The maximum number of tokens to generate.
 */
export type LLMConfig = {
  /**
   * @description LLM model name.
   * @type string
   */
  model: string;
  /**
   * @description The endpoint type for the model.
   * @type string
   */
  model_endpoint_type: LLMConfigModelEndpointTypeEnum;
  /**
   * @description The endpoint for the model.
   */
  model_endpoint?: string | null;
  /**
   * @description The wrapper for the model.
   */
  model_wrapper?: string | null;
  /**
   * @description The context window size for the model.
   * @type integer
   */
  context_window: number;
  /**
   * @description Puts \'inner_thoughts\' as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts.
   * @default true
   */
  put_inner_thoughts_in_kwargs?: boolean | null;
  /**
   * @description The handle for this config, in the format provider/model-name.
   */
  handle?: string | null;
  /**
   * @description The temperature to use when generating text with the model. A higher temperature will result in more random text.
   * @default 0.7
   * @type number | undefined
   */
  temperature?: number;
  /**
   * @description The maximum number of tokens to generate. If not set, the model will use its default value.
   * @default 4096
   */
  max_tokens?: number | null;
  /**
   * @description Whether or not the model should use extended thinking if it is a \'reasoning\' style model
   * @default false
   * @type boolean | undefined
   */
  enable_reasoner?: boolean;
  /**
   * @description Configurable thinking budget for extended thinking, only used if enable_reasoner is True. Minimum value is 1024.
   * @default 0
   * @type integer | undefined
   */
  max_reasoning_tokens?: number;
};
